{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "from os.path import join\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "################# Load and Save Data ################\n",
    "\n",
    "def load_json(rfdir, rfname):\n",
    "    with codecs.open(join(rfdir, rfname), 'r', encoding='utf-8') as rf:\n",
    "        return json.load(rf)\n",
    "\n",
    "\n",
    "def dump_json(obj, wfpath, wfname, indent=None):\n",
    "    with codecs.open(join(wfpath, wfname), 'w', encoding='utf-8') as wf:\n",
    "        json.dump(obj, wf, ensure_ascii=False, indent=indent)\n",
    "\n",
    "\n",
    "\n",
    "def dump_data(obj, wfpath, wfname):\n",
    "    with open(os.path.join(wfpath, wfname), 'wb') as wf:\n",
    "        pickle.dump(obj, wf)\n",
    "\n",
    "\n",
    "def load_data(rfpath, rfname):\n",
    "    with open(os.path.join(rfpath, rfname), 'rb') as rf:\n",
    "        return pickle.load(rf)\n",
    "\n",
    "    \n",
    "################# Random Walk ################\n",
    "\n",
    "import random\n",
    "class MetaPathGenerator:\n",
    "    def __init__(self):\n",
    "        self.paper_author = dict()\n",
    "        self.author_paper = dict()\n",
    "        self.paper_org = dict()\n",
    "        self.org_paper = dict()\n",
    "        self.paper_conf = dict()\n",
    "        self.conf_paper = dict()\n",
    "\n",
    "    def read_data(self, dirpath):\n",
    "        temp=set()\n",
    "\n",
    "        with open(dirpath + \"/paper_org.txt\", encoding='utf-8') as pafile:\n",
    "            for line in pafile:\n",
    "                temp.add(line)                       \n",
    "        for line in temp: \n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    p, a = toks[0], toks[1]\n",
    "                    if p not in self.paper_org:\n",
    "                        self.paper_org[p] = []\n",
    "                    self.paper_org[p].append(a)\n",
    "                    if a not in self.org_paper:\n",
    "                        self.org_paper[a] = []\n",
    "                    self.org_paper[a].append(p)\n",
    "        temp.clear()\n",
    "\n",
    "              \n",
    "        with open(dirpath + \"/paper_author.txt\", encoding='utf-8') as pafile:\n",
    "            for line in pafile:\n",
    "                temp.add(line)                       \n",
    "        for line in temp: \n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    p, a = toks[0], toks[1]\n",
    "                    if p not in self.paper_author:\n",
    "                        self.paper_author[p] = []\n",
    "                    self.paper_author[p].append(a)\n",
    "                    if a not in self.author_paper:\n",
    "                        self.author_paper[a] = []\n",
    "                    self.author_paper[a].append(p)\n",
    "        temp.clear()\n",
    "        \n",
    "                \n",
    "        with open(dirpath + \"/paper_conf.txt\", encoding='utf-8') as pcfile:\n",
    "            for line in pcfile:\n",
    "                temp.add(line)                       \n",
    "        for line in temp: \n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    p, a = toks[0], toks[1]\n",
    "                    if p not in self.paper_conf:\n",
    "                        self.paper_conf[p] = []\n",
    "                    self.paper_conf[p].append(a)\n",
    "                    if a not in self.conf_paper:\n",
    "                        self.conf_paper[a] = []\n",
    "                    self.conf_paper[a].append(p)\n",
    "        temp.clear()\n",
    "                    \n",
    "        print (\"#papers \", len(self.paper_conf))      \n",
    "        print (\"#authors\", len(self.author_paper))\n",
    "        print (\"#org_words\", len(self.org_paper))\n",
    "        print (\"#confs  \", len(self.conf_paper)) \n",
    "    \n",
    "    def generate_WMRW(self, outfilename, numwalks, walklength):\n",
    "        outfile = open(outfilename, 'w')\n",
    "        for paper0 in self.paper_conf: \n",
    "            for j in range(0, numwalks): #wnum walks\n",
    "                paper=paper0\n",
    "                outline = \"\"\n",
    "                i=0\n",
    "                while(i<walklength):\n",
    "                    i=i+1    \n",
    "                    if paper in self.paper_author:\n",
    "                        authors = self.paper_author[paper]\n",
    "                        numa = len(authors)\n",
    "                        authorid = random.randrange(numa)\n",
    "                        author = authors[authorid]\n",
    "                        \n",
    "                        papers = self.author_paper[author]\n",
    "                        nump = len(papers)\n",
    "                        if nump >1:\n",
    "                            paperid = random.randrange(nump)\n",
    "                            paper1 = papers[paperid]\n",
    "                            while paper1 == paper:\n",
    "                                paperid = random.randrange(nump)\n",
    "                                paper1 = papers[paperid]\n",
    "                            paper = paper1\n",
    "                            outline += \" \" + paper           \n",
    "                        \n",
    "                    if paper in self.paper_org:\n",
    "                        words = self.paper_org[paper]\n",
    "                        numw = len(words)\n",
    "                        wordid = random.randrange(numw) \n",
    "                        word = words[wordid]\n",
    "                    \n",
    "                        papers = self.org_paper[word]\n",
    "                        nump = len(papers)\n",
    "                        if nump >1:\n",
    "                            paperid = random.randrange(nump)\n",
    "                            paper1 = papers[paperid]\n",
    "                            while paper1 == paper:\n",
    "                                paperid = random.randrange(nump)\n",
    "                                paper1 = papers[paperid]\n",
    "                            paper = paper1\n",
    "                            outline += \" \" + paper  \n",
    "                    '''        \n",
    "                    if paper in self.paper_conf:\n",
    "                        words = self.paper_conf[paper]\n",
    "                        numw = len(words)\n",
    "                        wordid = random.randrange(numw) \n",
    "                        word = words[wordid]\n",
    "                    \n",
    "                        papers = self.conf_paper[word]\n",
    "                        nump = len(papers)\n",
    "                        if nump >1:\n",
    "                            paperid = random.randrange(nump)\n",
    "                            paper1 = papers[paperid]\n",
    "                            while paper1 == paper:\n",
    "                                paperid = random.randrange(nump)\n",
    "                                paper1 = papers[paperid]\n",
    "                            paper = paper1\n",
    "                            outline += \" \" + paper \n",
    "                    '''\n",
    "                outfile.write(outline + \"\\n\")\n",
    "        outfile.close()\n",
    "        \n",
    "        print (\"walks done\")\n",
    "        \n",
    "################# Compare Lists ################\n",
    "\n",
    "def tanimoto(p,q):\n",
    "    c = [v for v in p if v in q]\n",
    "    return float(len(c) / (len(p) + len(q) - len(c)))\n",
    "\n",
    "\n",
    "\n",
    "################# Paper similarity ################\n",
    "\n",
    "def generate_pair(pubs,outlier): ##求匹配相似度\n",
    "    dirpath = 'gene'\n",
    "    \n",
    "    paper_org = {}\n",
    "    paper_conf = {}\n",
    "    paper_author = {}\n",
    "    paper_word = {}\n",
    "    \n",
    "    temp=set()\n",
    "    with open(dirpath + \"/paper_org.txt\", encoding='utf-8') as pafile:\n",
    "        for line in pafile:\n",
    "            temp.add(line)                       \n",
    "    for line in temp: \n",
    "        toks = line.strip().split(\"\\t\")\n",
    "        if len(toks) == 2:\n",
    "            p, a = toks[0], toks[1]\n",
    "            if p not in paper_org:\n",
    "                paper_org[p] = []\n",
    "            paper_org[p].append(a)\n",
    "    temp.clear()\n",
    "    \n",
    "    with open(dirpath + \"/paper_conf.txt\", encoding='utf-8') as pafile:\n",
    "        for line in pafile:\n",
    "            temp.add(line)                       \n",
    "    for line in temp: \n",
    "        toks = line.strip().split(\"\\t\")\n",
    "        if len(toks) == 2:\n",
    "            p, a = toks[0], toks[1]\n",
    "            if p not in paper_conf:\n",
    "                paper_conf[p]=[]\n",
    "            paper_conf[p]=a\n",
    "    temp.clear()\n",
    "    \n",
    "    with open(dirpath + \"/paper_author.txt\", encoding='utf-8') as pafile:\n",
    "        for line in pafile:\n",
    "            temp.add(line)                       \n",
    "    for line in temp: \n",
    "        toks = line.strip().split(\"\\t\")\n",
    "        if len(toks) == 2:\n",
    "            p, a = toks[0], toks[1]\n",
    "            if p not in paper_author:\n",
    "                paper_author[p] = []\n",
    "            paper_author[p].append(a)\n",
    "    temp.clear()\n",
    "       \n",
    "    with open(dirpath + \"/paper_word.txt\", encoding='utf-8') as pafile:\n",
    "        for line in pafile:\n",
    "            temp.add(line)                       \n",
    "    for line in temp: \n",
    "        toks = line.strip().split(\"\\t\")\n",
    "        if len(toks) == 2:\n",
    "            p, a = toks[0], toks[1]\n",
    "            if p not in paper_word:\n",
    "                paper_word[p] = []\n",
    "            paper_word[p].append(a)\n",
    "    temp.clear()\n",
    "    \n",
    "    \n",
    "    paper_paper = np.zeros((len(pubs),len(pubs)))\n",
    "    for i,pid in enumerate(pubs):\n",
    "        if i not in outlier:\n",
    "            continue\n",
    "        for j,pjd in enumerate(pubs):\n",
    "            if j==i:\n",
    "                continue\n",
    "            ca=0\n",
    "            cv=0\n",
    "            co=0\n",
    "            ct=0\n",
    "          \n",
    "            if pid in paper_author and pjd in paper_author:\n",
    "                ca = len(set(paper_author[pid])&set(paper_author[pjd]))*1.5\n",
    "            if pid in paper_conf and pjd in paper_conf and 'null' not in paper_conf[pid]:\n",
    "                cv = tanimoto(set(paper_conf[pid]),set(paper_conf[pjd]))\n",
    "            if pid in paper_org and pjd in paper_org:\n",
    "                co = tanimoto(set(paper_org[pid]),set(paper_org[pjd]))\n",
    "            if pid in paper_word and pjd in paper_word:\n",
    "                ct = len(set(paper_word[pid])&set(paper_word[pjd]))/3\n",
    "                    \n",
    "            paper_paper[i][j] =ca+cv+co+ct\n",
    "            \n",
    "    return paper_paper\n",
    "\n",
    "    \n",
    "        \n",
    "################# Evaluate ################\n",
    "        \n",
    "def pairwise_evaluate(correct_labels,pred_labels):\n",
    "    TP = 0.0  # Pairs Correctly Predicted To SameAuthor\n",
    "    TP_FP = 0.0  # Total Pairs Predicted To SameAuthor\n",
    "    TP_FN = 0.0  # Total Pairs To SameAuthor\n",
    "\n",
    "    for i in range(len(correct_labels)):\n",
    "        for j in range(i + 1, len(correct_labels)):\n",
    "            if correct_labels[i] == correct_labels[j]:\n",
    "                TP_FN += 1\n",
    "            if pred_labels[i] == pred_labels[j]:\n",
    "                TP_FP += 1\n",
    "            if (correct_labels[i] == correct_labels[j]) and (pred_labels[i] == pred_labels[j]):\n",
    "                TP += 1\n",
    "\n",
    "    if TP == 0:\n",
    "        pairwise_precision = 0\n",
    "        pairwise_recall = 0\n",
    "        pairwise_f1 = 0\n",
    "    else:\n",
    "        pairwise_precision = TP / TP_FP\n",
    "        pairwise_recall = TP / TP_FN\n",
    "        pairwise_f1 = (2 * pairwise_precision * pairwise_recall) / (pairwise_precision + pairwise_recall)\n",
    "    return pairwise_precision, pairwise_recall, pairwise_f1\n",
    "\n",
    "\n",
    "################# Save Paper Features ################\n",
    "\n",
    "def save_relation(name_pubs_raw, name): # 保存论文的各种feature\n",
    "    name_pubs_raw = load_json('genename', name_pubs_raw)\n",
    "    ## trained by all text in the datasets. Training code is in the cells of \"train word2vec\"\n",
    "    save_model_name = \"word2vec/Aword2vec.model\"\n",
    "    model_w = word2vec.Word2Vec.load(save_model_name)\n",
    "    \n",
    "    r = '[!“”\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~—～’]+'\n",
    "    stopword = ['at','based','in','of','for','on','and','to','an','using','with','the','by','we','be','is','are','can']\n",
    "    stopword1 = ['university','univ','china','department','dept','laboratory','lab','school','al','et',\n",
    "                 'institute','inst','college','chinese','beijing','journal','science','international','arxiv']\n",
    "    org_stopword = ['at','based','in','of','for','on','and','to','using','with','the','by','we','be','is','are','can',\n",
    "                    'university','univ','china','department','dept','laboratory','lab','school','al','et',\n",
    "                 'institute','inst','college','beijing']\n",
    "                    #'university','univ','china','department','dept','institute','inst','laboratory','lab','technology',\n",
    "                    #'al','et','science','sciences','school','chinese','college']\n",
    "    \n",
    "    f1 = open ('gene/paper_author.txt','w',encoding = 'utf-8')\n",
    "    f2 = open ('gene/paper_conf.txt','w',encoding = 'utf-8')\n",
    "    f3 = open ('gene/paper_word.txt','w',encoding = 'utf-8')\n",
    "    f4 = open ('gene/paper_org.txt','w',encoding = 'utf-8')\n",
    "\n",
    "    \n",
    "    taken = name.split(\"_\")\n",
    "    name = taken[0] + taken[1]\n",
    "    name_reverse = taken[1]  + taken[0]\n",
    "    if len(taken)>2:\n",
    "        name = taken[0] + taken[1] + taken[2]\n",
    "        name_reverse = taken[2]  + taken[0] + taken[1]\n",
    "    \n",
    "    authorname_dict={}\n",
    "    ptext_emb = {}  \n",
    "    \n",
    "    tcp=set()  \n",
    "    for i,pid in enumerate(name_pubs_raw):\n",
    "        \n",
    "        pub = name_pubs_raw[pid]\n",
    "        \n",
    "        #save authors\n",
    "        org=\"\"\n",
    "        for author in pub[\"authors\"]:\n",
    "            authorname = re.sub(r,'', author[\"name\"]).lower()\n",
    "            taken = authorname.split(\" \")\n",
    "            if len(taken)==2: ##检测目前作者名是否在作者词典中\n",
    "                authorname = taken[0] + taken[1]\n",
    "                authorname_reverse = taken[1]  + taken[0] \n",
    "            \n",
    "                if authorname not in authorname_dict:\n",
    "                    if authorname_reverse not in authorname_dict:\n",
    "                        authorname_dict[authorname]=1\n",
    "                    else:\n",
    "                        authorname = authorname_reverse \n",
    "            else:\n",
    "                authorname = authorname.replace(\" \",\"\")\n",
    "            \n",
    "            if authorname!=name and authorname!=name_reverse:\n",
    "                f1.write(pid + '\\t' + authorname + '\\n')\n",
    "        \n",
    "            else:\n",
    "                if \"org\" in author:\n",
    "                    org = author[\"org\"]\n",
    "                    \n",
    "                    \n",
    "        #save org 待消歧作者的机构名\n",
    "        pstr = org.strip()\n",
    "        pstr = pstr.lower() #小写\n",
    "        pstr = re.sub(r,' ', pstr) #去除符号\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip() #去除多余空格\n",
    "        pstr = pstr.split(' ')\n",
    "        pstr = [word for word in pstr if len(word)>1]\n",
    "        pstr = [word for word in pstr if word not in org_stopword]\n",
    "        #pstr = [word for word in pstr if word not in stopword]\n",
    "        #pstr = [word for word in pstr if word not in stopword1]\n",
    "        pstr=set(pstr)\n",
    "        for word in pstr:\n",
    "            f4.write(pid + '\\t' + word + '\\n')\n",
    "\n",
    "        \n",
    "        #save venue\n",
    "        pstr = pub[\"venue\"].strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        pstr = pstr.split(' ')\n",
    "        pstr = [word for word in pstr if len(word)>1]\n",
    "        pstr = [word for word in pstr if word not in stopword1]\n",
    "        pstr = [word for word in pstr if word not in stopword]\n",
    "        for word in pstr:\n",
    "            f2.write(pid + '\\t' + word + '\\n')\n",
    "        if len(pstr)==0:\n",
    "            f2.write(pid + '\\t' + 'null' + '\\n')\n",
    "\n",
    "            \n",
    "        #save text\n",
    "        pstr = \"\"    \n",
    "        keyword=\"\"\n",
    "        if \"keywords\" in pub:\n",
    "            for word in pub[\"keywords\"]:\n",
    "                keyword=keyword+word+\" \"\n",
    "        pstr = pstr + pub[\"title\"]\n",
    "        pstr=pstr.strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        pstr = pstr.split(' ')\n",
    "        pstr = [word for word in pstr if len(word)>1]\n",
    "        pstr = [word for word in pstr if word not in stopword]\n",
    "        for word in pstr:\n",
    "            f3.write(pid + '\\t' + word + '\\n')\n",
    "        \n",
    "        #save all words' embedding\n",
    "        #pstr = keyword + \" \" + pub[\"title\"] + \" \" + pub[\"venue\"] + \" \" + org\n",
    "        pstr = keyword + \" \" + pub[\"title\"] + \" \" + pub[\"venue\"] + \" \" + org\n",
    "        if \"year\" in pub:\n",
    "              pstr = pstr +  \" \" + str(pub[\"year\"])\n",
    "        pstr=pstr.strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        pstr = pstr.split(' ')\n",
    "        pstr = [word for word in pstr if len(word)>1]\n",
    "        pstr = [word for word in pstr if word not in stopword]\n",
    "        pstr = [word for word in pstr if word not in stopword1]\n",
    "        #print (pstr)\n",
    "\n",
    "        words_vec=[]\n",
    "        for word in pstr:\n",
    "            if (word in model_w):\n",
    "                words_vec.append(model_w[word])\n",
    "        if len(words_vec)<1:\n",
    "            words_vec.append(np.zeros(100))\n",
    "            tcp.add(i)\n",
    "            #print ('outlier:',pid,pstr)\n",
    "        ptext_emb[pid] = np.mean(words_vec,0)\n",
    "        \n",
    "    #  ptext_emb: key is paper id, and the value is the paper's text embedding\n",
    "    dump_data(ptext_emb,'gene','ptext_emb.pkl')\n",
    "    # the paper index that lack text information\n",
    "    dump_data(tcp,'gene','tcp.pkl')\n",
    "            \n",
    " \n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    f3.close()\n",
    "    f4.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE all text in the datasets\n",
    "\n",
    "import codecs\n",
    "import json\n",
    "from os.path import join\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "pubs_raw = load_json(\"train\",\"train_pub.json\")\n",
    "pubs_raw1 = load_json(\"sna_data\",\"sna_valid_pub.json\")\n",
    "#pubs_raw2 = load_json(\"sna_test_data\",\"test_pub_sna.json\")\n",
    "r = '[!“”\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~—～’]+'\n",
    "\n",
    "f1 = open ('gene/all_text.txt','w',encoding = 'utf-8')\n",
    "\n",
    "for i,pid in enumerate(pubs_raw):\n",
    "    pub = pubs_raw[pid]\n",
    "    \n",
    "    for author in pub[\"authors\"]:\n",
    "        if \"org\" in author:\n",
    "                org = author[\"org\"]\n",
    "                pstr = org.strip()\n",
    "                pstr = pstr.lower()\n",
    "                pstr = re.sub(r,' ', pstr)\n",
    "                pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "                f1.write(pstr+'\\n')\n",
    "            \n",
    "    title = pub[\"title\"]\n",
    "    pstr=title.strip()\n",
    "    pstr = pstr.lower()\n",
    "    pstr = re.sub(r,' ', pstr)\n",
    "    pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "    f1.write(pstr+'\\n')\n",
    "    \n",
    "    keyword=\"\"\n",
    "    if \"keywords\" in pub:\n",
    "        for word in pub[\"keywords\"]:\n",
    "            keyword=keyword+word+\" \"\n",
    "    pstr=keyword.strip()\n",
    "    pstr = pstr.lower()\n",
    "    pstr = re.sub(r,' ', pstr)\n",
    "    pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "    f1.write(pstr+'\\n')\n",
    "    \n",
    "    if \"abstract\" in pub and type(pub[\"abstract\"]) is str:\n",
    "        abstract = pub[\"abstract\"]\n",
    "        pstr=abstract.strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        f1.write(pstr+'\\n')\n",
    "        \n",
    "    venue = pub[\"venue\"]\n",
    "    pstr=venue.strip()\n",
    "    pstr = pstr.lower()\n",
    "    pstr = re.sub(r,' ', pstr)\n",
    "    pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "    f1.write(pstr+'\\n')\n",
    "    \n",
    "for i,pid in enumerate(pubs_raw1):\n",
    "    pub = pubs_raw1[pid]\n",
    "    \n",
    "    for author in pub[\"authors\"]:\n",
    "        if \"org\" in author:\n",
    "                org = author[\"org\"]\n",
    "                pstr = org.strip()\n",
    "                pstr = pstr.lower()\n",
    "                pstr = re.sub(r,' ', pstr)\n",
    "                pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "                f1.write(pstr+'\\n')\n",
    "            \n",
    "    title = pub[\"title\"]\n",
    "    pstr=title.strip()\n",
    "    pstr = pstr.lower()\n",
    "    pstr = re.sub(r,' ', pstr)\n",
    "    pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "    f1.write(pstr+'\\n')\n",
    "    \n",
    "    keyword=\"\"\n",
    "    if \"keywords\" in pub:\n",
    "        for word in pub[\"keywords\"]:\n",
    "            keyword=keyword+word+\" \"\n",
    "    pstr=keyword.strip()\n",
    "    pstr = pstr.lower()\n",
    "    pstr = re.sub(r,' ', pstr)\n",
    "    pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "    f1.write(pstr+'\\n')\n",
    "    \n",
    "    if \"abstract\" in pub and type(pub[\"abstract\"]) is str:\n",
    "        abstract = pub[\"abstract\"]\n",
    "        pstr=abstract.strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        f1.write(pstr+'\\n')\n",
    "        \n",
    "    venue = pub[\"venue\"]\n",
    "    pstr=venue.strip()\n",
    "    pstr = pstr.lower()\n",
    "    pstr = re.sub(r,' ', pstr)\n",
    "    pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "    f1.write(pstr+'\\n')\n",
    "\n",
    "'''\n",
    "for i,pid in enumerate(pubs_raw2):\n",
    "    pub = pubs_raw2[pid]\n",
    "    \n",
    "    for author in pub[\"authors\"]:\n",
    "        if \"org\" in author:\n",
    "                org = author[\"org\"]\n",
    "                pstr = org.strip()\n",
    "                pstr = pstr.lower()\n",
    "                pstr = re.sub(r,' ', pstr)\n",
    "                pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "                f1.write(pstr+'\\n')\n",
    "            \n",
    "    title = pub[\"title\"]\n",
    "    pstr=title.strip()\n",
    "    pstr = pstr.lower()\n",
    "    pstr = re.sub(r,' ', pstr)\n",
    "    pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "    f1.write(pstr+'\\n')\n",
    "    \n",
    "    if \"abstract\" in pub and type(pub[\"abstract\"]) is str:\n",
    "        abstract = pub[\"abstract\"]\n",
    "        pstr=abstract.strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        f1.write(pstr+'\\n')\n",
    "        \n",
    "    venue = pub[\"venue\"]\n",
    "    pstr=venue.strip()\n",
    "    pstr = pstr.lower()\n",
    "    pstr = re.sub(r,' ', pstr)\n",
    "    pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "    f1.write(pstr+'\\n')\n",
    "'''        \n",
    "        \n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "sentences = word2vec.Text8Corpus(r'gene/all_text.txt')\n",
    "model = word2vec.Word2Vec(sentences, size=100,negative =3, min_count=2, window=3)\n",
    "model.save('word2vec/Aword2vec.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# name disambiguation (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import word2vec\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "pubs_raw = load_json(\"sna_test_data\",\"test_pub_sna.json\")\n",
    "name_pubs1 = load_json(\"sna_test_data\",\"example_evaluation_scratch.json\")\n",
    "\n",
    "result={}\n",
    "\n",
    "for n,name in enumerate(name_pubs1):\n",
    "    pubs=[]\n",
    "    for cluster in name_pubs1[name]:\n",
    "        pubs.extend(cluster)\n",
    "    \n",
    "    \n",
    "    print (n,name,len(pubs))\n",
    "    if len(pubs)==0:\n",
    "        result[name]=[]\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    ##保存关系\n",
    "    ###############################################################\n",
    "    name_pubs_raw = {}\n",
    "    for i,pid in enumerate(pubs):\n",
    "        name_pubs_raw[pid] = pubs_raw[pid]\n",
    "        \n",
    "    dump_json(name_pubs_raw, 'genename', name+'.json', indent=4)\n",
    "    save_relation(name+'.json', name)  \n",
    "    ###############################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##元路径游走类\n",
    "    ###############################################################r\n",
    "    mpg = MetaPathGenerator()\n",
    "    mpg.read_data(\"gene\")\n",
    "    ###############################################################\n",
    "    \n",
    "\n",
    "    \n",
    "    ##论文关系表征向量\n",
    "    ############################################################### \n",
    "    all_embs=[]\n",
    "    rw_num = 10\n",
    "    cp=set()\n",
    "    for k in range(rw_num):\n",
    "        mpg.generate_WMRW(\"gene/RW.txt\",5,20)\n",
    "        sentences = word2vec.Text8Corpus(r'gene/RW.txt')\n",
    "        model = word2vec.Word2Vec(sentences, size=100,negative =25, min_count=1, window=10)\n",
    "        embs=[]\n",
    "        for i,pid in enumerate(pubs):\n",
    "            if pid in model:\n",
    "                embs.append(model[pid])\n",
    "            else:\n",
    "                cp.add(i)\n",
    "                embs.append(np.zeros(100))\n",
    "        all_embs.append(embs)\n",
    "    all_embs= np.array(all_embs)\n",
    "    print ('relational outlier:',cp)    \n",
    "    ############################################################### \n",
    " \n",
    "\n",
    "\n",
    "    ##论文文本表征向量\n",
    "    ###############################################################  \n",
    "    ptext_emb=load_data('gene','ptext_emb.pkl')\n",
    "    tcp=load_data('gene','tcp.pkl')\n",
    "    print ('semantic outlier:',tcp)\n",
    "    tembs=[]\n",
    "    for i,pid in enumerate(pubs):\n",
    "        tembs.append(ptext_emb[pid])\n",
    "    ###############################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##论文相似性矩阵\n",
    "    ###############################################################\n",
    "    sk_sim = np.zeros((len(pubs),len(pubs)))\n",
    "    for k in range(rw_num):\n",
    "        sk_sim = sk_sim + pairwise_distances(all_embs[k],metric=\"cosine\")\n",
    "    sk_sim =sk_sim/rw_num    \n",
    "    \n",
    "\n",
    "    tembs = pairwise_distances(tembs,metric=\"cosine\")\n",
    "   \n",
    "    w=1\n",
    "    sim = (np.array(sk_sim) + w*np.array(tembs))/(1+w)\n",
    "    ############################################################### \n",
    "    \n",
    "    \n",
    "  \n",
    "    ##evaluate\n",
    "    ###############################################################\n",
    "    pre = DBSCAN(eps = 0.2, min_samples = 4,metric =\"precomputed\").fit_predict(sim)\n",
    "    pre= np.array(pre)\n",
    "    \n",
    "    \n",
    "    ##离群论文集\n",
    "    outlier=set()\n",
    "    for i in range(len(pre)):\n",
    "        if pre[i]==-1:\n",
    "            outlier.add(i)\n",
    "    for i in cp:\n",
    "        outlier.add(i)\n",
    "    for i in tcp:\n",
    "        outlier.add(i)\n",
    "            \n",
    "        \n",
    "    ##基于阈值的相似性匹配\n",
    "    paper_pair = generate_pair(pubs,outlier)\n",
    "    paper_pair1 = paper_pair.copy()\n",
    "    K = len(set(pre))\n",
    "    for i in range(len(pre)):\n",
    "        if i not in outlier:\n",
    "            continue\n",
    "        j = np.argmax(paper_pair[i])\n",
    "        while j in outlier:\n",
    "            paper_pair[i][j]=-1\n",
    "            j = np.argmax(paper_pair[i])\n",
    "        if paper_pair[i][j]>=1.5:\n",
    "            pre[i]=pre[j]\n",
    "        else:\n",
    "            pre[i]=K\n",
    "            K=K+1\n",
    "    \n",
    "    for ii,i in enumerate(outlier):\n",
    "        for jj,j in enumerate(outlier):\n",
    "            if jj<=ii:\n",
    "                continue\n",
    "            else:\n",
    "                if paper_pair1[i][j]>=1.5:\n",
    "                    pre[j]=pre[i]\n",
    "            \n",
    "    \n",
    "\n",
    "    print (pre,len(set(pre)))\n",
    "    \n",
    "    result[name]=[]\n",
    "    for i in set(pre):\n",
    "        oneauthor=[]\n",
    "        for idx,j in enumerate(pre):\n",
    "            if i == j:\n",
    "                oneauthor.append(pubs[idx])\n",
    "        result[name].append(oneauthor)\n",
    "    \n",
    "\n",
    "dump_json(result, \"genetest\", \"result_test.json\",indent =4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pubs_raw = load_json(\"train\",\"train_pub.json\")\n",
    "name_pubs = load_json(\"train\",\"train_author.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# name disambiguation (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import word2vec\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "\n",
    "result=[]\n",
    "for n,name in enumerate(name_pubs):\n",
    "    ilabel=0\n",
    "    pubs=[] # all papers\n",
    "    labels=[] # ground truth\n",
    "    \n",
    "    for author in name_pubs[name]:\n",
    "        iauthor_pubs = name_pubs[name][author]\n",
    "        for pub in iauthor_pubs:\n",
    "            pubs.append(pub)\n",
    "            labels.append(ilabel)\n",
    "        ilabel += 1\n",
    "        \n",
    "    print (n,name,len(pubs))\n",
    "    \n",
    "    \n",
    "    if len(pubs)==0:\n",
    "        result.append(0)\n",
    "        continue\n",
    "    \n",
    "    ##保存关系\n",
    "    ###############################################################\n",
    "    name_pubs_raw = {}\n",
    "    for i,pid in enumerate(pubs):\n",
    "        name_pubs_raw[pid] = pubs_raw[pid]\n",
    "        \n",
    "    dump_json(name_pubs_raw, 'genename', name+'.json', indent=4)\n",
    "    save_relation(name+'.json', name)  \n",
    "    ###############################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##元路径游走类\n",
    "    ###############################################################r\n",
    "    mpg = MetaPathGenerator()\n",
    "    mpg.read_data(\"gene\")\n",
    "    ###############################################################\n",
    "    \n",
    "  \n",
    "    \n",
    "    ##论文关系表征向量\n",
    "    ############################################################### \n",
    "    all_embs=[]\n",
    "    rw_num =1\n",
    "    cp=set()\n",
    "    for k in range(rw_num):\n",
    "        mpg.generate_WMRW(\"gene/RW.txt\",3,30) #生成路径集\n",
    "        sentences = word2vec.Text8Corpus(r'gene/RW.txt')\n",
    "        model = word2vec.Word2Vec(sentences, size=100,negative =20, min_count=1, window=10)\n",
    "        embs=[]\n",
    "        for i,pid in enumerate(pubs):\n",
    "            if pid in model:\n",
    "                embs.append(model[pid])\n",
    "            else:\n",
    "                cp.add(i)\n",
    "                embs.append(np.zeros(100))\n",
    "        all_embs.append(embs)\n",
    "    all_embs= np.array(all_embs)\n",
    "    print ('relational outlier:',cp)\n",
    "    ############################################################### \n",
    "\n",
    "    \n",
    "    \n",
    "    ##论文文本表征向量\n",
    "    ###############################################################   \n",
    "    ptext_emb=load_data('gene','ptext_emb.pkl')\n",
    "    tcp=load_data('gene','tcp.pkl')\n",
    "    print ('semantic outlier:',tcp)\n",
    "    tembs=[]\n",
    "    for i,pid in enumerate(pubs):\n",
    "        tembs.append(ptext_emb[pid])\n",
    "    ############################################################### \n",
    "    \n",
    "    ##离散点\n",
    "    outlier=set()\n",
    "    for i in cp:\n",
    "        outlier.add(i)\n",
    "    for i in tcp:\n",
    "        outlier.add(i)\n",
    "    \n",
    "    ##网络嵌入向量相似度\n",
    "    sk_sim = np.zeros((len(pubs),len(pubs)))\n",
    "    for k in range(rw_num):\n",
    "        sk_sim = sk_sim + pairwise_distances(all_embs[k],metric=\"cosine\")\n",
    "    sk_sim =sk_sim/rw_num \n",
    "\n",
    "    \n",
    "    ##文本相似度\n",
    "    t_sim = pairwise_distances(tembs,metric=\"cosine\")\n",
    "    \n",
    "    w=0.5\n",
    "    sim = (np.array(sk_sim) + w*np.array(t_sim))/(1+w)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##evaluate\n",
    "    ###############################################################\n",
    "    pre = DBSCAN(eps = 0.15, min_samples = 3,metric =\"precomputed\").fit_predict(sim)\n",
    "    \n",
    "    \n",
    "    for i in range(len(pre)):\n",
    "        if pre[i]==-1:\n",
    "            outlier.add(i)\n",
    "    \n",
    "    ## assign each outlier a label\n",
    "    paper_pair = generate_pair(pubs,outlier)\n",
    "    paper_pair1 = paper_pair.copy()\n",
    "    K = len(set(pre))\n",
    "    for i in range(len(pre)):\n",
    "        if i not in outlier:\n",
    "            continue\n",
    "        j = np.argmax(paper_pair[i])\n",
    "        while j in outlier:\n",
    "            paper_pair[i][j]=-1\n",
    "            j = np.argmax(paper_pair[i])\n",
    "        if paper_pair[i][j]>=1.5:\n",
    "            pre[i]=pre[j]\n",
    "        else:\n",
    "            pre[i]=K\n",
    "            K=K+1\n",
    "    \n",
    "    ## find nodes in outlier is the same label or not\n",
    "    for ii,i in enumerate(outlier):\n",
    "        for jj,j in enumerate(outlier):\n",
    "            if jj<=ii:\n",
    "                continue\n",
    "            else:\n",
    "                if paper_pair1[i][j]>=1.5:\n",
    "                    pre[j]=pre[i]\n",
    "            \n",
    "            \n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    pre = np.array(pre)\n",
    "    print (labels,len(set(labels)))\n",
    "    print (pre,len(set(pre)))\n",
    "    pairwise_precision, pairwise_recall, pairwise_f1 = pairwise_evaluate(labels,pre)\n",
    "    print (pairwise_precision, pairwise_recall, pairwise_f1)\n",
    "    result.append(pairwise_f1)\n",
    "\n",
    "    print ('avg_f1:', np.mean(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# name disambiguation (valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import word2vec\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "pubs_raw = load_json(\"sna_data\",\"sna_valid_pub.json\")\n",
    "name_pubs1 = load_json(\"sna_data\",\"sna_valid_example_evaluation_scratch.json\")\n",
    "\n",
    "result={}\n",
    "\n",
    "for n,name in enumerate(name_pubs1):\n",
    "    pubs=[]\n",
    "    for cluster in name_pubs1[name]:\n",
    "        pubs.extend(cluster)\n",
    "    \n",
    "    \n",
    "    print (n,name,len(pubs))\n",
    "    if len(pubs)==0:\n",
    "        result[name]=[]\n",
    "        continue\n",
    "    \n",
    "    \n",
    "     ##保存关系\n",
    "    ###############################################################\n",
    "    name_pubs_raw = {}\n",
    "    for i,pid in enumerate(pubs):\n",
    "        name_pubs_raw[pid] = pubs_raw[pid]\n",
    "        \n",
    "    dump_json(name_pubs_raw, 'genename', name+'.json', indent=4)\n",
    "    save_relation(name+'.json', name)  \n",
    "    ###############################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##元路径游走类\n",
    "    ###############################################################r\n",
    "    mpg = MetaPathGenerator()\n",
    "    mpg.read_data(\"gene\")\n",
    "    ###############################################################\n",
    "    \n",
    "  \n",
    "    \n",
    "    ##论文关系表征向量\n",
    "    ############################################################### \n",
    "    all_embs=[]\n",
    "    rw_num =10\n",
    "    cp=set()\n",
    "    for k in range(rw_num):\n",
    "        mpg.generate_WMRW(\"gene/RW.txt\",3,30) #生成路径集\n",
    "        sentences = word2vec.Text8Corpus(r'gene/RW.txt')\n",
    "        model = word2vec.Word2Vec(sentences, size=100,negative =20, min_count=1, window=10)\n",
    "        embs=[]\n",
    "        for i,pid in enumerate(pubs):\n",
    "            if pid in model:\n",
    "                embs.append(model[pid])\n",
    "            else:\n",
    "                cp.add(i)\n",
    "                embs.append(np.zeros(100))\n",
    "        all_embs.append(embs)\n",
    "    all_embs= np.array(all_embs)\n",
    "    print ('relational outlier:',cp)\n",
    "    ############################################################### \n",
    "\n",
    "    \n",
    "    \n",
    "    ##论文文本表征向量\n",
    "    ###############################################################   \n",
    "    ptext_emb=load_data('gene','ptext_emb.pkl')\n",
    "    tcp=load_data('gene','tcp.pkl')\n",
    "    print ('semantic outlier:',tcp)\n",
    "    tembs=[]\n",
    "    for i,pid in enumerate(pubs):\n",
    "        tembs.append(ptext_emb[pid])\n",
    "    ############################################################### \n",
    "    \n",
    "    ##离散点\n",
    "    outlier=set()\n",
    "    for i in cp:\n",
    "        outlier.add(i)\n",
    "    for i in tcp:\n",
    "        outlier.add(i)\n",
    "    \n",
    "    ##网络嵌入向量相似度\n",
    "    sk_sim = np.zeros((len(pubs),len(pubs)))\n",
    "    for k in range(rw_num):\n",
    "        sk_sim = sk_sim + pairwise_distances(all_embs[k],metric=\"cosine\")\n",
    "    sk_sim =sk_sim/rw_num    \n",
    "    \n",
    "    ##文本相似度\n",
    "    t_sim = pairwise_distances(tembs,metric=\"cosine\")\n",
    "    \n",
    "    w=0.5\n",
    "    sim = (np.array(sk_sim) + w*np.array(t_sim))/(1+w)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##evaluate\n",
    "    ###############################################################\n",
    "    pre = DBSCAN(eps = 0.15, min_samples = 3,metric =\"precomputed\").fit_predict(sim)\n",
    "    \n",
    "    \n",
    "    for i in range(len(pre)):\n",
    "        if pre[i]==-1:\n",
    "            outlier.add(i)\n",
    "    \n",
    "    ## assign each outlier a label\n",
    "    paper_pair = generate_pair(pubs,outlier)\n",
    "    paper_pair1 = paper_pair.copy()\n",
    "    K = len(set(pre))\n",
    "    for i in range(len(pre)):\n",
    "        if i not in outlier:\n",
    "            continue\n",
    "        j = np.argmax(paper_pair[i])\n",
    "        while j in outlier:\n",
    "            paper_pair[i][j]=-1\n",
    "            j = np.argmax(paper_pair[i])\n",
    "        if paper_pair[i][j]>=1.5:\n",
    "            pre[i]=pre[j]\n",
    "        else:\n",
    "            pre[i]=K\n",
    "            K=K+1\n",
    "    \n",
    "    ## find nodes in outlier is the same label or not\n",
    "    for ii,i in enumerate(outlier):\n",
    "        for jj,j in enumerate(outlier):\n",
    "            if jj<=ii:\n",
    "                continue\n",
    "            else:\n",
    "                if paper_pair1[i][j]>=1.5:\n",
    "                    pre[j]=pre[i]\n",
    "            \n",
    "    \n",
    "\n",
    "    print (pre,len(set(pre)))\n",
    "    \n",
    "    result[name]=[]\n",
    "    for i in set(pre):\n",
    "        oneauthor=[]\n",
    "        for idx,j in enumerate(pre):\n",
    "            if i == j:\n",
    "                oneauthor.append(pubs[idx])\n",
    "        result[name].append(oneauthor)\n",
    "    \n",
    "\n",
    "dump_json(result, \"genetest\", \"result_valid.json\",indent =4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
